# CUDA Configuration for NVIDIA GPUs
# Optimized for modern NVIDIA GPUs with Tensor Cores

# Device configuration
device:
  prefer_mps: false
  dtype: bfloat16  # Preferred for modern NVIDIA GPUs
  fallback_to_cpu: true

# Model configuration
model:
  name: "EleutherAI/pythia-410m-deduped"
  num_labels: 4
  max_length: 512  # Larger context for CUDA

# LoRA configuration
lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "dense", "fc", "proj"]

# Training configuration
training:
  output_dir: "outputs/cuda_experiment"
  epochs: 2
  batch_size: 4  # Larger batch size for CUDA
  gradient_accumulation_steps: 8  # Effective batch size = 32
  learning_rate: 0.00015
  weight_decay: 0.01
  warmup_steps: 100
  logging_steps: 10
  eval_steps: 100
  save_steps: 500
  gradient_checkpointing: true  # Enable for memory efficiency
  use_class_weights: true
  # CUDA-specific optimizations
  amp: true  # Automatic Mixed Precision
  compile_model: false  # Set to true for PyTorch 2.0+ with torch.compile

# Data configuration
data:
  source: "synthetic"  # synthetic or ag_news
  synthetic_path: "data/synthetic_tickets.jsonl"
  limit: 2000
  test_size: 0.1
  val_size: 0.1
  date_column: null  # Use random split

# Preprocessing configuration
preprocessing:
  lowercase: false
  strip_html: true
  redact_pii: true

# Logging configuration
log_level: "INFO"
seed: 42

# CUDA-specific notes:
# - bfloat16 provides better numerical stability than float16
# - Tensor Cores accelerate mixed-precision training
# - Gradient checkpointing reduces memory usage
# - torch.compile can provide additional speedup (PyTorch 2.0+)
# - For Triton serving, export merged weights to TorchScript/ONNX
